{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a6504df",
   "metadata": {},
   "source": [
    "## Business problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9d70b8",
   "metadata": {},
   "source": [
    "Port terminals constantly strive to improve the efficiency of their operations through a careful\n",
    "management of their berth facilities, machinery and personnel. \n",
    "\n",
    "\n",
    "\n",
    "The most important variables when planning terminal operations is knowing which vessels will arrive at the terminal and when.\n",
    "\n",
    "\n",
    "MarineTraffic aims at being the best visibility providers by providing up to date vessel tracking data (using AIS), as well as additional derived information such as the estimated time of arrival (ETA) of a vessel to a port of interest.\n",
    "\n",
    "\n",
    "AIS messages contain information on the port that the vessel is traveling to as well as the estimated time of arrival.\n",
    "\n",
    "However, since ports may consist of more than one terminal, the exact terminal that the\n",
    "vessel will visit is not known in advance making it difficult for MarineTraffic to assign future arrivals to terminals which, in turn, limits the ability to measure terminal congestion and calculate more accurate terminal arrival times.\n",
    "\n",
    "\n",
    "\n",
    "A model which predicts the terminal a vessel will travel to has the potential to help all parties involved in a port call to plan their operations more effectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7b62ca",
   "metadata": {},
   "source": [
    "# Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27dc5d4",
   "metadata": {},
   "source": [
    "A dataset has been extracted containing container calls at terminals that took place during the past 3\n",
    "years for the Port of **Hamburg and Port of Los Angeles**. The dataset contains the following fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d61f2b",
   "metadata": {},
   "source": [
    "A dataset has been extracted containing container calls at terminals that took place during the past 3\n",
    "years for the Port of Hamburg and Port of Los Angeles. The dataset contains the following fields;\n",
    "\n",
    "\n",
    "* **last_port**: Port where the *last* terminal call by vessel is recorded.\n",
    "\n",
    "\n",
    "\n",
    "* **last_terminal**: The immediately previous *terminal* call of the vessel.\n",
    "\n",
    "\n",
    "* **last_terminal_doc_timestamp**: Timestamp of previous terminal call.\n",
    "\n",
    "\n",
    "* **current_port**: Port where the current terminal call by vessel is recorded.\n",
    "\n",
    "\n",
    "* **current_terminal**: Current terminal call of the vessel.\n",
    "\n",
    "\n",
    "* **shipname**: Name of the vessel.\n",
    "\n",
    "\n",
    "* **dock_timestamp**: Timestamp of current terminal call.\n",
    "\n",
    "\n",
    "* **GRT**: Vessel capacity (gross tonnage unit).\n",
    "\n",
    "\n",
    "* **TEU**: Vessel capacity (twenty-foot equivalent Unit).\n",
    "\n",
    "\n",
    "* **length**: Vessel length.\n",
    "\n",
    "\n",
    "* **width**: Vessel width."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c7a156",
   "metadata": {},
   "source": [
    "# Goals and Deliverable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409a947",
   "metadata": {},
   "source": [
    "The goal of this task is to implement & evaluate the accuracy of a solution that predicts the terminal\n",
    "that a vessel will call.\n",
    "**The product team claims that the history of terminals visited by a vessel in the past is a critical factor\n",
    "that should be incorporated into the model**.\n",
    "\n",
    "Some important steps that your solution would be expected to address and describe are the following:\n",
    "* What features have you finally selected and engineered for your modeling approach? What led you to these choices? Why & how have you processed them?\n",
    "* Which features seem to be the most important & how did you evaluate their importance?\n",
    "* Do your findings agree with the product team’s insights discussed above? Before developing a ML model, how would you evaluate the importance/predictive power of one of the productidentified features as an independent variable?\n",
    "* What type of prediction/training model have you chosen and why?\n",
    "* How well does your predictive solution perform in terms of predicting the terminal a vessel will call?\n",
    "* What different metrics/graphs can you use in order to understand when & why the algorithm fails/succeeds?\n",
    "* What would be your baseline (i.e. a “naive” approach) to compare against?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee228f63",
   "metadata": {},
   "source": [
    "# Solution Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf6c265",
   "metadata": {},
   "source": [
    "### problem understanding & EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8111499a",
   "metadata": {},
   "source": [
    "In order to understand the problem we have to dive into the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6203351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from scikitplot.estimators import plot_learning_curve\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a550ed8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(y_test, y_pred, label_no=11):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Normalize the confusion matrix\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Define the labels for the x and y axes\n",
    "    labels = list(range(label_no))\n",
    "\n",
    "    # Create the heatmap\n",
    "    sns.heatmap(cm_norm, annot=cm, cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.title('Classification Heatmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3459be",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"data/mt_terminal_calls.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e2d7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312a2c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2e0a6f",
   "metadata": {},
   "source": [
    "##### lets analyse the ports a bit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da38c94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'ships to HAMBURG : {raw_data.loc[raw_data[\"current_port\"]==\"HAMBURG\",:].shape[0]}')\n",
    "print(f'ships to LA : {raw_data.loc[raw_data[\"current_port\"]!=\"HAMBURG\",:].shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e70d216",
   "metadata": {},
   "source": [
    "I will create a new feature: a ratio of GRT/(length*width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a0e32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data[\"doc_timestamp\"] = pd.to_datetime(raw_data.loc[:, \"doc_timestamp\"])\n",
    "raw_data[\"last_terminal_doc_timestamp\"] = pd.to_datetime(raw_data.loc[:, \"last_terminal_doc_timestamp\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc8131",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data[\"diff\"] = raw_data.loc[:, \"doc_timestamp\"] - raw_data.loc[:, \"last_terminal_doc_timestamp\"]\n",
    "raw_data[\"diff\"] = raw_data.apply(lambda x: x['diff'].seconds,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fe3b2e",
   "metadata": {},
   "source": [
    "Some extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dabdfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data[\"ratio\"] = raw_data.loc[:, \"grt\"]/(raw_data.loc[:,\"width\"]*raw_data.loc[:,\"length\"])\n",
    "raw_data[\"foot_ratio\"] = raw_data.loc[:, \"teu\"]/(raw_data.loc[:,\"width\"]*raw_data.loc[:,\"length\"])\n",
    "\n",
    "raw_data[\"size\"] = raw_data.loc[:,\"width\"]*raw_data.loc[:,\"length\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d63737",
   "metadata": {},
   "source": [
    "ratio feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe561d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.loc[raw_data[\"current_port\"]==\"HAMBURG\",\"ratio\"].hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34a636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.loc[raw_data[\"current_port\"]!=\"HAMBURG\",\"ratio\"].hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b64904",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.loc[:,\"ratio\"].hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeba8c7",
   "metadata": {},
   "source": [
    "size feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900f7546",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.loc[raw_data[\"current_port\"]!=\"HAMBURG\",\"size\"].hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e17932",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.loc[raw_data[\"current_port\"]!=\"HAMBURG\",[\"size\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381fcd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.loc[:,\"size\"].hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec760fe7",
   "metadata": {},
   "source": [
    "clearly much different distribution per port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b87849",
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_ham = raw_data.loc[raw_data[\"current_port\"]==\"HAMBURG\", \"current_terminal\"].unique().tolist()\n",
    "terminal_la = raw_data.loc[raw_data[\"current_port\"]==\"LOS ANGELES\", \"current_terminal\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8248eafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def id2port(x, th, tl):\n",
    "    if x[\"current_port\"] == \"HAMBURG\":\n",
    "        idx = th[x[\"current_terminal\"]]\n",
    "    else:\n",
    "        idx = tl[x[\"current_terminal\"]]\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91395796",
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_ham = {i:idx for idx, i in enumerate(terminal_ham)}\n",
    "terminal_la = {i:idx for idx, i in enumerate(terminal_la)}\n",
    "raw_data[\"id_on_port\"] =  raw_data.apply(id2port, args=(terminal_ham, terminal_la), axis=1)\n",
    "la_distribution = raw_data.loc[raw_data[\"current_port\"]!=\"HAMBURG\", [\"width\",\"length\",\"size\",\"id_on_port\"]]\n",
    "# Create hexbin plot with Seaborn\n",
    "sns.jointplot(data=la_distribution, x=\"width\",y=\"length\",hue=\"id_on_port\",)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d14cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hamburg_distibution = raw_data.loc[raw_data[\"current_port\"]==\"HAMBURG\", [\"width\",\"length\",\"size\",\"id_on_port\"]]\n",
    "# Create hexbin plot with Seaborn\n",
    "sns.jointplot(data=hamburg_distibution, x=\"width\",y=\"length\",hue=\"id_on_port\",  color=\"#4CB391\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7f6d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "nu_cols = raw_data.select_dtypes(include=['float64','int64']).columns\n",
    "print(nu_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab01245",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(raw_data.loc[raw_data[\"current_port\"]==\"HAMBURG\", nu_cols],hue=\"id_on_port\", palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28912170",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(raw_data.loc[raw_data[\"current_port\"]!=\"HAMBURG\", nu_cols],hue=\"id_on_port\", palette=\"Set3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb89aa4",
   "metadata": {},
   "source": [
    "# Construct baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f9e04c",
   "metadata": {},
   "source": [
    "#### use previous terminal as a prediction for the next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbae9a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "gather_results = dict()\n",
    "gather_data = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf288e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_ship = raw_data.groupby([\"shipname\",\"current_port\"])\n",
    "restructure = []\n",
    "matches = 0\n",
    "for ship_no,(name,group) in enumerate(grouped_by_ship):\n",
    "    restructure.append(group.sort_values(by=\"last_terminal_doc_timestamp\"))\n",
    "    restructure[-1].loc[:, \"concat\"] = restructure[-1].loc[:,\"id_on_port\"].astype(str) +\"__\" + restructure[-1].loc[:,\"current_port\"]\n",
    "    restructure[-1].loc[:, \"concat_s\"] = restructure[-1].loc[:, \"concat\"].shift()\n",
    "    tmp = restructure[-1].dropna()\n",
    "    if tmp.dropna().shape[0]:\n",
    "        matches += tmp.dropna().loc[restructure[-1][\"concat\"]==restructure[-1][\"concat_s\"], :].shape[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38e86e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches, raw_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e0d2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches/raw_data.shape[0] # this here is essentially the accuracy metric on the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c94449d",
   "metadata": {},
   "source": [
    "#### explanation of the main metrics that we will use since this problem is multi class classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c86851",
   "metadata": {},
   "source": [
    "Micro F1 and Macro F1 scores are two commonly used metrics to evaluate the performance of a multi-class classification model.\n",
    "\n",
    "Micro F1 score considers all the predictions and ground truth values from all classes, and gives an equal weight to each sample. This means that the overall F1 score is calculated by taking into account the total true positives, false positives, and false negatives across all classes. Micro F1 is useful when you want to put equal emphasis on each individual sample in the evaluation.\n",
    "\n",
    "Macro F1 score, on the other hand, calculates the F1 score for each class and takes the unweighted mean of the F1 scores. In this case, each class is given equal importance, regardless of the number of samples in each class. Macro F1 is useful when you care about the overall performance of the model across all classes, rather than the performance on individual samples.\n",
    "\n",
    "So, to summarize, if you want to put equal emphasis on each sample, use Micro F1. If you care about the overall performance of the model across all classes, use Macro F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdacc6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "restructure = pd.concat(restructure)\n",
    "print(restructure.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef3966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "population = Counter(restructure[\"concat\"])\n",
    "pprint(sorted([(value,key) for key, value in population.items()]))\n",
    "print(\"HAMBURG\")\n",
    "pprint(sorted([(value,key) for key, value in population.items() if \"HAM\" in key]))\n",
    "print(\"LA\")\n",
    "pprint(sorted([(value,key) for key, value in population.items() if \"HAM\" not in key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d8dc25",
   "metadata": {},
   "source": [
    "In our case we have a clear majority classifier especially on 4 of the HAMBURG ports, but our main objective is not to\n",
    "place the ships equally in the port but rather predict their true destination. Hence we will give very slightly more weight to micro f1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000031b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(restructure.shape[0])\n",
    "restructure.loc[restructure[\"concat_s\"].isna(), \"concat_s\"] = \"no history\"\n",
    "print(\"macro f1\",f1_score(restructure[\"concat\"], restructure[\"concat_s\"],average='macro'))\n",
    "print(\"micro f1\", f1_score(restructure[\"concat\"], restructure[\"concat_s\"],average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ab94a2",
   "metadata": {},
   "source": [
    "This is the first and extremely naive baseline since it does not take into account the prior knowledge of the port"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b903be11",
   "metadata": {},
   "source": [
    "#### Next steps to reach the final solution\n",
    "\n",
    "1. Verify/reject the markets team theory that historical data of the ship work.\n",
    "\n",
    "2. Create more features by using polynomial ones in order to help catch a non linear relationship between the features and the actual label\n",
    "\n",
    "3. In the procedure of solving the problem i will use essentially 19 possible labels ( 8 LA terminals + 11 HAMBURG terminals) and i will show all the increments before i reach my final result.\n",
    "\n",
    "4. Reduce the set of labels by introducing the \"port\" feature as a binary feature (since we have a priori knowledge). This way we could simplify the problem, and in the case of a missclassification it could easily be rectified with postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d317cb3",
   "metadata": {},
   "source": [
    "### we need to create the feature that keeps the history track of the ship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a3191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "restructure.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cccc6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "restructure[\"doc_timestamp\"] = pd.to_datetime(restructure.loc[:, \"doc_timestamp\"])\n",
    "restructure[\"last_terminal_doc_timestamp\"] = pd.to_datetime(restructure.loc[:, \"last_terminal_doc_timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c73cc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_ship = restructure.groupby([\"shipname\"])\n",
    "# here *we don't* need to groupby the port as well, we just need the historical data.\n",
    "make_history = []\n",
    "matches = 0\n",
    "for ship_no,(name,group) in enumerate(grouped_by_ship):\n",
    "    group = group.sort_values(by=\"last_terminal_doc_timestamp\")    \n",
    "    increment = list()\n",
    "    historical_data = group.loc[:, \"concat\"].to_list()\n",
    "    trajectory = list()\n",
    "    number_of_trips = list(range(len(historical_data)))\n",
    "    for item in historical_data:\n",
    "        if not len(increment):\n",
    "            trajectory.append([\"no history\"])\n",
    "        else:\n",
    "            trajectory.append([])\n",
    "            trajectory[-1].extend(increment)\n",
    "        increment.append(item)\n",
    "    group[\"history\"] = trajectory\n",
    "    group[\"no_trips\"] = number_of_trips\n",
    "    trips_on_the_current_port = [1 if y[-1] in x[-1] else 0 for x,y in zip(trajectory, list(group[\"current_port\"]))]\n",
    "    tptp = [1 if y[-1] in x[-1] else 0 for x,y in zip(trajectory, list(group[\"current_port\"]))]\n",
    "#     group[\"trips_on_this_port\"] = [sum(tptp[:idx]) for idx, i in enumerate(tptp,1)]\n",
    "    \n",
    "    sm = list()\n",
    "    for i in trajectory:\n",
    "        if \"no history\" in i[-1] and len(i)==1:\n",
    "            sm.append(0)\n",
    "        elif \"no history\" not in i:\n",
    "            sm.append(len(set(i)))\n",
    "    group[\"different_terminals\"] = sm#[0 if (('no history' in i[-1]) and (len(i[-1])==1)) else len(set(i[-1])) for i in trajectory ]\n",
    "    group[\"last_visit\"] = [\"no history\" if \"history\" in i[-1] else i[-1] for i in trajectory]\n",
    "    group[\"last_port_visit\"] = group[\"concat_s\"].copy()\n",
    "    group[\"parallel\"] = historical_data\n",
    "    make_history.append(group)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67488bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_history = pd.concat(make_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16986824",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "make_history[\"parallel\"] = encoder.fit_transform(make_history[\"parallel\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1660d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_history.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b023c690",
   "metadata": {},
   "source": [
    "now we effectively/easily use this dataset or a subset of it (we don't need ALL the features anymore) and tackle this as a timeseries problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d4fbe5",
   "metadata": {},
   "source": [
    "as we said before it is important to have the port as a feature since we're expected to know the port the ship is going"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a2a9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_history[\"current_port\"] = make_history.apply(lambda x: 0 if \"HAM\" in x[\"current_port\"] else 1,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038a6456",
   "metadata": {},
   "source": [
    "the last ports and the last terminals don't actually matter. What actually matters is the difference between the current port call and the previous port call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55274347",
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_history = make_history.copy()\n",
    "# dataframe cleanup\n",
    "del make_history[\"last_port\"]\n",
    "del make_history[\"last_terminal\"]\n",
    "# del make_history[\"doc_timestamp\"]\n",
    "del make_history[\"last_terminal_doc_timestamp\"]\n",
    "del make_history[\"concat_s\"]\n",
    "\n",
    "enc = OrdinalEncoder()\n",
    "make_history.loc[:, [\"shipname\"]] = enc.fit_transform(make_history.loc[:, [\"shipname\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62fb4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(make_history.loc[:, \"last_visit\"])\n",
    "one_hot_columns = list(one_hot.columns)\n",
    "\n",
    "one_hot_2 = pd.get_dummies(make_history.loc[:, \"last_port_visit\"])\n",
    "one_hot_2 = one_hot_2.add_suffix(\"oh2\")\n",
    "one_hot_columns_2 = list(one_hot_2.columns)\n",
    "\n",
    "\n",
    "extra_columns = one_hot_columns +  one_hot_columns_2\n",
    "\n",
    "make_history = pd.concat([make_history, one_hot, one_hot_2],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22791282",
   "metadata": {},
   "source": [
    "\n",
    "this WILL NOT BE USED AS A FEATURE but rather as a medium to be able to process the data a bit easier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7315263",
   "metadata": {},
   "source": [
    "the shipname cannot be possibly used as a feature because\n",
    "it will create curse of dimentonality since we can only use one hot\n",
    "representations. Unless of course we could use ship embeddings (which would)\n",
    "take a bigger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db3f8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_history.loc[:, \"last_visit\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb208790",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_history.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a589dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_tr, x_te, y_tr, y_te = [],[],[],[]\n",
    "# for ship in make_history.loc[:,\"shipname\"].unique():\n",
    "#     # they are already sorted by date\n",
    "#     tmp = make_history.loc[make_history[\"shipname\"]==ship,:].copy().sort_values(by=\"doc_timestamp\")\n",
    "\n",
    "#     X = tmp.loc[:,tmp.columns!=\"id_on_port\"].copy()\n",
    "#     del X[\"shipname\"]\n",
    "#     y = tmp.loc[tmp[\"shipname\"]==ship, [\"id_on_port\"]].copy()\n",
    "\n",
    "#     if X.shape[0]>=5: # anything less than 5 samples doesn't even matter if it get in the training dataset. (apart from the obvious fact that you cant split less than 5 samples)\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "#         x_tr.append(X_train)\n",
    "#         x_te.append(X_test)\n",
    "\n",
    "#         y_tr.append(y_train)\n",
    "#         y_te.append(y_test)\n",
    "#     else:\n",
    "#         x_te.append(X)\n",
    "#         y_te.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cba237",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_history = make_history.sort_values(by=\"doc_timestamp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4271022",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_history[\"quarter\"] = make_history.loc[:, \"doc_timestamp\"].dt.quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc4c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_history[\"day of week\"] = make_history.loc[:, \"doc_timestamp\"].dt.dayofweek\n",
    "make_history[\"year quarter\"] = make_history.loc[:, \"doc_timestamp\"].dt.quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c756bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_main, X_test_main, y_train, y_test= train_test_split(make_history.loc[:,make_history.columns!=\"id_on_port\"], make_history.loc[:,[\"id_on_port\"]], test_size=0.2, shuffle=False)# pd.concat(x_tr), pd.concat(x_te), pd.concat(y_tr), pd.concat(y_te)\n",
    "\n",
    "y_train_main = y_train.copy()\n",
    "y_test_main = y_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6bd4f5",
   "metadata": {},
   "source": [
    "X_train_main and X_test_main is a superset of all the features so we can run with any subset. The whole dataset is splitted in a way to solve as a timeseries problem per ship (even if we don't use the ship as a feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce822b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = ['no_trips','current_port', 'grt', 'teu', 'length', 'width', 'diff', 'ratio', 'size', 'different_terminals','foot_ratio']\n",
    "print(f\" features used: {ft}\")\n",
    "\n",
    "X_train = X_train_main.loc[:, ft].copy()\n",
    "X_test = X_test_main.loc[:, ft].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fe12e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[:, ['grt', 'teu', 'length', 'width', 'diff', 'ratio', 'foot_ratio', 'size',]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c5449d",
   "metadata": {},
   "source": [
    "### phase 2 of experiments with these features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b198af",
   "metadata": {},
   "source": [
    "# logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f13e1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=42)\n",
    "\n",
    "svt = RobustScaler()\n",
    "\n",
    "model.fit(svt.fit_transform(X_train),y_train)\n",
    "\n",
    "svtest = RobustScaler()\n",
    "\n",
    "y_pred = model.predict(svtest.fit_transform(X_test))\n",
    "\n",
    "print(\"micro\", f1_score(y_pred,y_test,average='micro'))\n",
    "print(\"macro\", f1_score(y_pred,y_test,average='macro'))\n",
    "print(classification_report(y_pred.ravel(),y_test.values.ravel()))\n",
    "\n",
    "gather_results[\"lr_ft\"] = ({\"micro f1\":f1_score(y_pred,y_test,average='micro'), \"macro\":f1_score(y_pred,y_test,average='macro')})\n",
    "\n",
    "gather_data[\"lr_ft\"] = {\"X_train\":svt.fit_transform(X_train),\"y_train\":y_train,\"X_test\":svtest.fit_transform(X_test),\"y_test\":y_test,\"model\":model, \"y_pred\":y_pred}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493a2514",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723960ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"micro\", f1_score(y_pred, y_test.values.ravel(),average='micro'))\n",
    "print(\"macro\", f1_score(y_pred, y_test.values.ravel(),average='macro'))\n",
    "print(classification_report(y_pred.ravel(),y_test.values.ravel()))\n",
    "gather_results[\"rfr_ft\"] = {\"micro f1\":f1_score(y_pred,y_test,average='micro'), \"macro\":f1_score(y_pred,y_test,average='macro')}\n",
    "\n",
    "gather_data[\"rfr_ft\"] = {\"X_train\":X_train,\"y_train\":y_train,\"X_test\":X_test,\"y_test\":y_test,\"model\":model, \"y_pred\":y_pred}\n",
    "\n",
    "heatmap(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90535cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df = pd.DataFrame(model.feature_importances_.reshape(1,-1), columns=X_train.columns).T\n",
    "check_df.columns = [\"importance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27130bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df.sort_values(by=\"importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ec0e23",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5247021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVMs' need scaling before used.\n",
    "model = SVC(random_state=42)\n",
    "\n",
    "\n",
    "svt = RobustScaler()\n",
    "\n",
    "model.fit(svt.fit_transform(X_train),y_train)\n",
    "\n",
    "svtest = RobustScaler()\n",
    "\n",
    "y_pred = model.predict(svtest.fit_transform(X_test))\n",
    "\n",
    "print(\"micro\", f1_score(y_pred.ravel(),y_test.values.ravel(),average='micro'))\n",
    "print(\"macro\", f1_score(y_pred.ravel(),y_test.values.ravel(),average='macro'))\n",
    "print(classification_report(y_pred.ravel(),y_test.values.ravel()))\n",
    "gather_results[\"svm_ft\"] = {\"micro f1\":f1_score(y_pred,y_test,average='micro'), \"macro\":f1_score(y_pred,y_test,average='macro')}\n",
    "\n",
    "\n",
    "gather_data[\"svm_ft\"] = {\"X_train\":X_train,\"y_train\":y_train,\"X_test\":X_test,\"y_test\":y_test,\"model\":model, \"y_pred\":y_pred}\n",
    "\n",
    "heatmap(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26d4498",
   "metadata": {},
   "outputs": [],
   "source": [
    "checking_df = pd.concat([X_test,y_test],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72133cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "checking_df.loc[(checking_df[\"current_port\"]==1) & (checking_df[\"id_on_port\"]>7),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0617629",
   "metadata": {},
   "source": [
    "Of course here we have cases of misclassification, and using post process we can place correct the prediction of the 9 and above.\n",
    "Terminal for port of Los Angeles to any of the other 8 ports that are available.\n",
    "\n",
    "1. This could happen in multiple ways, either by using the knowledge of the prior distribution and place the prediction of over 8th terminal to the most visited terminal of the correct port.\n",
    "2. If we had information about the terminals (aka similiarties of the terminals i would try to correlate the 9 to 11 terminals to one of the other 8.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee6d2a1",
   "metadata": {},
   "source": [
    "# compare with baseline on the same validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc589bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "restructure2 = restructure.loc[restructure.index.isin(X_test.index), :].copy()\n",
    "print(restructure2.shape[0])\n",
    "print(\"macro f1\",f1_score(restructure2[\"concat\"], restructure2[\"concat_s\"],average='macro'))\n",
    "print(\"micro f1\", f1_score(restructure2[\"concat\"], restructure2[\"concat_s\"],average='micro'))\n",
    "\n",
    "gather_results[\"base_vali\"] = {\"micro f1\":f1_score(restructure2[\"concat\"], restructure2[\"concat_s\"],average='micro'), \"macro\":f1_score(restructure2[\"concat\"], restructure2[\"concat_s\"],average='macro')}\n",
    "\n",
    "print(classification_report(restructure2[\"concat_s\"], restructure2[\"concat\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af38acd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "restructure2.loc[restructure2[\"concat_s\"]==\"no history\",:].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1486e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one of the reasons that we get a better result from the baseline is that we do have a majority classifier on our hands "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15081e0",
   "metadata": {},
   "source": [
    "# Evidently we need to explore new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adeb1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first of all we must definitely should use the last_known terminal\n",
    "# hence veifying the hypothesis of the product team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf5f9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_main.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eea894",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_main.loc[X_train_main[\"last_visit\"]==\"no history\",:].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec9c044",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_main.loc[X_train_main[\"last_visit\"]==\"no history\",:].shape[0],X_test_main.loc[X_test_main[\"last_visit\"]==\"no history\",:].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9584af",
   "metadata": {},
   "source": [
    "should i use the previous lag i would have to delete 685 + 712 rows out of the dataset since they don't actually have history, instead i have decided to make that a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458d162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_cols = ft.copy()\n",
    "expanded_cols.extend(one_hot_columns)\n",
    "expanded_cols.extend(one_hot_columns_2)\n",
    "X_train = X_train_main.loc[:, expanded_cols].copy()\n",
    "X_test = X_test_main.loc[:, expanded_cols].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91be0215",
   "metadata": {},
   "source": [
    "### Logistic regression with expanded set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b870b5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=42)\n",
    "\n",
    "svt = RobustScaler()\n",
    "\n",
    "model.fit(svt.fit_transform(X_train),y_train)\n",
    "\n",
    "svtest = RobustScaler()\n",
    "\n",
    "y_pred = model.predict(svtest.fit_transform(X_test))\n",
    "\n",
    "print(\"micro\", f1_score(y_pred,y_test,average='micro'))\n",
    "print(\"macro\", f1_score(y_pred,y_test,average='macro'))\n",
    "print(classification_report(y_pred.ravel(),y_test.values.ravel()))\n",
    "\n",
    "gather_results[\"lr_expanded\"] = {\"micro f1\":f1_score(y_pred,y_test,average='micro'), \"macro\":f1_score(y_pred,y_test,average='macro')}\n",
    "\n",
    "gather_data[\"lr_expanded\"] = {\"X_train\":X_train,\"y_train\":y_train,\"X_test\":X_test,\"y_test\":y_test,\"model\":model, \"y_pred\":y_pred}\n",
    "\n",
    "heatmap(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bc2c8b",
   "metadata": {},
   "source": [
    "### RandomForestClassifier with expanded set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad080350",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"micro\", f1_score(y_pred, y_test.values.ravel(),average='micro'))\n",
    "print(\"macro\", f1_score(y_pred, y_test.values.ravel(),average='macro'))\n",
    "print(classification_report(y_pred.ravel(),y_test.values.ravel()))\n",
    "gather_results[\"rfr_expanded\"] = {\"micro f1\":f1_score(y_pred,y_test,average='micro'), \"macro\":f1_score(y_pred,y_test,average='macro')}\n",
    "\n",
    "gather_data[\"rfr_expanded\"] = {\"X_train\":X_train,\"y_train\":y_train,\"X_test\":X_test,\"y_test\":y_test,\"model\":model, \"y_pred\":y_pred}\n",
    "\n",
    "heatmap(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f38b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "md = RandomForestClassifier(random_state=42)\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit()\n",
    "tscv = TimeSeriesSplit(n_splits=3, test_size=round(0.1*len(X_train)), gap=2)\n",
    "\n",
    "plot_learning_curve(md, X=X_train, y=y_train, scoring='f1_micro', cv=tscv, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c3e105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it seems that with more training examples the model actually learns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b722a8dd",
   "metadata": {},
   "source": [
    "### SVMClassifier with expanded set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b51ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVMs' need scaling before used.\n",
    "model = SVC(random_state=42)\n",
    "\n",
    "\n",
    "svt = RobustScaler()\n",
    "\n",
    "model.fit(svt.fit_transform(X_train),y_train)\n",
    "\n",
    "svtest = RobustScaler()\n",
    "\n",
    "y_pred = model.predict(svtest.fit_transform(X_test))\n",
    "\n",
    "print(\"micro\", f1_score(y_pred.ravel(),y_test.values.ravel(),average='micro'))\n",
    "print(\"macro\", f1_score(y_pred.ravel(),y_test.values.ravel(),average='macro'))\n",
    "print(classification_report(y_pred.ravel(),y_test.values.ravel()))\n",
    "gather_results[\"svm_expanded\"] = {\"micro f1\":f1_score(y_pred,y_test,average='micro'), \"macro\":f1_score(y_pred,y_test,average='macro')}\n",
    "\n",
    "gather_data[\"svm_expanded\"] = {\"X_train\":svt.fit_transform(X_train),\"y_train\":y_train,\"X_test\":svtest.fit_transform(X_test),\"y_test\":y_test,\"model\":model, \"y_pred\":y_pred}\n",
    "\n",
    "heatmap(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afefb87",
   "metadata": {},
   "source": [
    "All of those models could use hyperparameter tuning by using GridSearchCV (with cv disabled because we have a sort of a timeseries). But i thought it would make more sense to produce more features in order to get more insights for the problem. The EDA process is more valuable while tuning can be fully automated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f71f2a9",
   "metadata": {},
   "source": [
    "We can expand our feature search a bit more by using Polynomial features (without the bias) and clustering of the features\n",
    "to give us an extra insight for data of our ships\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7795e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of inertia values for different k values\n",
    "inertias = []\n",
    "k_values = range(1, 11)\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_train)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow graph\n",
    "plt.plot(k_values, inertias, '-o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for KMeans Clustering')\n",
    "plt.xticks(np.arange(1, 11))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a96bd9",
   "metadata": {},
   "source": [
    "as we can see from the plot the best number of clusters seems to be number 3 (it looks more like an elbow)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f04475",
   "metadata": {},
   "source": [
    "so we will test this extra feature on our best performing algorithm so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3856ce14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_main.shape,y_train_main.shape)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "res = kmeans.fit_predict(X_train)\n",
    "X_train[\"res\"] = res\n",
    "\n",
    "tmp1 = pd.get_dummies(X_train[\"res\"])\n",
    "tmp1 = tmp1.add_suffix(\"oh2\")\n",
    "\n",
    "X_train = pd.concat([X_train,tmp1],axis=1)\n",
    "del X_train[\"res\"]\n",
    "\n",
    "res = kmeans.predict(X_test)\n",
    "X_test[\"res\"] = res\n",
    "tmp2 = pd.get_dummies(X_test[\"res\"])\n",
    "tmp2 = tmp2.add_suffix(\"oh2\")\n",
    "X_test = pd.concat([X_test,tmp2],axis=1)\n",
    "del X_test[\"res\"]\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"micro\", f1_score(y_pred, y_test.values.ravel(),average='micro'))\n",
    "print(\"macro\", f1_score(y_pred, y_test.values.ravel(),average='macro'))\n",
    "print(classification_report(y_pred.ravel(),y_test.values.ravel()))\n",
    "gather_results[\"rfr_cluster\"] = {\"micro f1\":f1_score(y_pred,y_test,average='micro'), \"macro\":f1_score(y_pred,y_test,average='macro')}\n",
    "\n",
    "\n",
    "gather_data[\"rfr_cluster\"] = {\"X_train\":X_train,\"y_train\":y_train,\"X_test\":X_test,\"y_test\":y_test,\"model\":model, \"y_pred\":y_pred}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdc2fbc",
   "metadata": {},
   "source": [
    "Same test for polynomial features. The polynomial features help in capturing non linear relationships.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8868b168",
   "metadata": {},
   "source": [
    "### add Polynomial features + one hot encoding features + clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6f3baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"features used\")\n",
    "print(list(X_test.columns))\n",
    "degree = 2\n",
    "\n",
    "# Create a PolynomialFeatures object\n",
    "poly = PolynomialFeatures(degree, include_bias=False)\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_poly = poly.fit_transform(X_train.loc[:, ft])\n",
    "\n",
    "X_train_poly = pd.concat([pd.DataFrame(X_train_poly).reset_index(drop=True),X_train.loc[:, extra_columns].reset_index(drop=True),tmp1.reset_index(drop=True)],axis=1)\n",
    "X_train_poly.columns = [f\"lala_{i}\" for i in list(X_train_poly.columns)]\n",
    "# Transform the test data using the same polynomial features\n",
    "X_test_poly = poly.transform(X_test.loc[:, ft])\n",
    "X_test_poly = pd.concat([pd.DataFrame(X_test_poly),X_test.loc[:, extra_columns].reset_index(drop=True),tmp2.reset_index(drop=True)],axis=1)\n",
    "X_test_poly.columns = X_train_poly.columns\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "print(X_train_poly.shape)\n",
    "model.fit(X_train_poly,y_train.astype(float))\n",
    "\n",
    "y_pred = model.predict(X_test_poly)\n",
    "\n",
    "print(\"micro\", f1_score(y_pred, y_test.values.ravel(),average='micro'))\n",
    "print(\"macro\", f1_score(y_pred, y_test.values.ravel(),average='macro'))\n",
    "print(classification_report(y_pred.ravel(),y_test.values.ravel()))\n",
    "gather_results[\"rfr_poly_cluster_expanded\"] = {\"micro f1\":f1_score(y_pred,y_test,average='micro'), \"macro\":f1_score(y_pred,y_test,average='macro')}\n",
    "\n",
    "\n",
    "\n",
    "gather_data[\"rfr_poly_cluster_expanded\"] = {\"X_train\":X_train_poly,\"y_train\":y_train,\"X_test\":X_test_poly,\"y_test\":y_test,\"model\":model, \"y_pred\":y_pred}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134edfa7",
   "metadata": {},
   "source": [
    "### add Polynomial features + one hot encoding features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0460cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 2\n",
    "\n",
    "# Create a PolynomialFeatures object\n",
    "poly = PolynomialFeatures(degree, include_bias=False)\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_poly = poly.fit_transform(X_train.loc[:, ft])\n",
    "\n",
    "X_train_poly = pd.concat([pd.DataFrame(X_train_poly).reset_index(drop=True),X_train.loc[:, extra_columns].reset_index(drop=True)],axis=1)\n",
    "X_train_poly.columns = [f\"lala_{i}\" for i in list(X_train_poly.columns)]\n",
    "# Transform the test data using the same polynomial features\n",
    "X_test_poly = poly.transform(X_test.loc[:, ft])\n",
    "X_test_poly = pd.concat([pd.DataFrame(X_test_poly),X_test.loc[:, extra_columns].reset_index(drop=True)],axis=1)\n",
    "X_test_poly.columns = X_train_poly.columns\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "print(X_train_poly.shape)\n",
    "model.fit(X_train_poly,y_train.astype(float))\n",
    "\n",
    "y_pred = model.predict(X_test_poly)\n",
    "\n",
    "print(\"micro\", f1_score(y_pred, y_test.values.ravel(),average='micro'))\n",
    "print(\"macro\", f1_score(y_pred, y_test.values.ravel(),average='macro'))\n",
    "print(classification_report(y_pred.ravel(),y_test.values.ravel()))\n",
    "gather_results[\"rfr_expanded_poly\"] = {\"micro f1\":f1_score(y_pred,y_test,average='micro'), \"macro\":f1_score(y_pred,y_test,average='macro')}\n",
    "\n",
    "gather_data[\"rfr_expanded_poly\"] = {\"X_train\":X_train_poly,\"y_train\":y_train,\"X_test\":X_test_poly,\"y_test\":y_test,\"model\":model, \"y_pred\":y_pred}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5f0447",
   "metadata": {},
   "source": [
    "# Finally we can perform feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0785d5b",
   "metadata": {},
   "source": [
    "Using backward and forward elimination we can get the best subset of features that we can use. The thing is this procedure is extremely computationally expensive and i didn't manage the get the result of those functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eab843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_elimination(X, y, model, metric):\n",
    "    best_features = list(X.columns)\n",
    "    best_score = 0\n",
    "    \n",
    "    for k in tqdm(range(1, len(X.columns) + 1)):\n",
    "        for subset in combinations(X.columns, k):\n",
    "            model.fit(X.loc[:, list(subset)], y)\n",
    "            y_pred = model.predict(X[list(subset)])\n",
    "            score = metric(y, y_pred.ravel(), average='macro')\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_features = list(subset)\n",
    "                \n",
    "    return best_features\n",
    "\n",
    "def forward_selection(X, y, model, metric):\n",
    "    best_features = []\n",
    "    best_score = 0\n",
    "    \n",
    "    for k in tqdm(range(1, len(X.columns) + 1)):\n",
    "        if k == 1:\n",
    "            candidate_features = list(X.columns)\n",
    "        else:\n",
    "            candidate_features = [f for f in X.columns if f not in best_features]\n",
    "        \n",
    "        for feature in candidate_features:\n",
    "            model.fit(X[best_features + [feature]], y)\n",
    "            y_pred = model.predict(X[best_features + [feature]])\n",
    "            score = metric(y, y_pred.ravel(), average='macro')\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_features.append(feature)\n",
    "                \n",
    "    return best_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209b2f57",
   "metadata": {},
   "source": [
    "### the product theory has been verified that the historical data been extremely useful on the next prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22124084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put example of non timeseries data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22b5652",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aba5b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449f1e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data[\"concat\"] = raw_data[\"current_port\"] + raw_data[\"current_terminal\"]\n",
    "\n",
    "X = restructure.loc[:, [\"grt\",\"teu\",\"length\", \"width\", \"diff\", \"ratio\"]].copy()\n",
    "Y = restructure.loc[:, [\"concat\"]].copy()\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "Y = encoder.fit_transform(Y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"micro f1\",f1_score(y_test, y_pred, average=\"micro\"))\n",
    "print(\"macro f1\",f1_score(y_test, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0485c8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the indices and compare with previous baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a3fede",
   "metadata": {},
   "outputs": [],
   "source": [
    "restructure = restructure.loc[restructure.index.isin(X_test.index), :]\n",
    "print(\"micro f1\", f1_score(restructure[\"concat_s\"],restructure[\"concat\"],average='micro' ))\n",
    "print(\"macro f1\",f1_score(restructure[\"concat_s\"], restructure[\"concat\"],average='macro' ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f30e65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape, restructure.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96822d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(restructure[\"concat_s\"], restructure[\"concat\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5214f683",
   "metadata": {},
   "source": [
    "# labels distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7333bda9",
   "metadata": {},
   "source": [
    "0. (1, '10HAMBURG')\n",
    "1. (1, '7LOS ANGELES')\n",
    "2. (1, '9HAMBURG')\n",
    "3. (9, '8HAMBURG')\n",
    "4. (28, '7HAMBURG')\n",
    "5. (40, '6HAMBURG')\n",
    "6. (157, '4HAMBURG')\n",
    "7. (225, '2LOS ANGELES')\n",
    "8. (266, '1HAMBURG')\n",
    "9. (271, '4LOS ANGELES')\n",
    "10. (323, '6LOS ANGELES')\n",
    "11. (355, '5LOS ANGELES')\n",
    "12. (469, '3LOS ANGELES')\n",
    "13. (485, '1LOS ANGELES')\n",
    "14. (514, '0LOS ANGELES')\n",
    "15. (1183, '5HAMBURG')\n",
    "16. (2313, '2HAMBURG')\n",
    "17. (2574, '0HAMBURG')\n",
    "18. (2839, '3HAMBURG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac32ca0",
   "metadata": {},
   "source": [
    "##### check with  19 labels on best performing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c76ac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gather_data[\"svm_expanded\"][\"model\"]\n",
    "\n",
    "X_train = gather_data[\"svm_expanded\"][\"X_train\"]\n",
    "X_test = gather_data[\"svm_expanded\"][\"X_test\"]\n",
    "\n",
    "y_train = X_train_main.loc[:,\"parallel\"]\n",
    "y_test = X_test_main.loc[:,\"parallel\"]\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"macro\", f1_score(y_pred, y_test.values.ravel(),average='macro'))\n",
    "print(\"micro\", f1_score(y_pred, y_test.values.ravel(),average='micro'))\n",
    "\n",
    "gather_results[\"svm_19_labels\"] = {\"micro f1\":f1_score(y_pred,y_test,average='micro'), \"macro\":f1_score(y_pred,y_test,average='macro')}\n",
    "\n",
    "gather_data[\"svm_19_labels\"] = {\"X_train\":X_train,\"y_train\":y_train,\"X_test\":X_test,\"y_test\":y_test,\"model\":model, \"y_pred\":y_pred}\n",
    "\n",
    "\n",
    "print(classification_report(y_pred.ravel(),y_test_main.values.ravel()))\n",
    "\n",
    "heatmap(y_test, y_pred, 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a4c430",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ft))\n",
    "sc = RobustScaler()\n",
    "X = X_train_main.loc[:, ft].copy().copy()\n",
    "X = sc.fit_transform(X)\n",
    "y = X_train_main.loc[:, \"parallel\"].copy().copy()\n",
    "\n",
    "predictor = model\n",
    "backward_best_features = backward_elimination(X, y, predictor, f1_score)\n",
    "print(\"Best features using backward elimination:\", backward_best_features)\n",
    "\n",
    "forward_best_features = forward_selection(X, y, predictor, f1_score)\n",
    "print(\"Best features using forward selection:\", forward_best_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca0212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gather_data[\"lr_expanded\"][\"model\"]\n",
    "\n",
    "X_train = gather_data[\"lr_expanded\"][\"X_train\"]\n",
    "X_test = gather_data[\"lr_expanded\"][\"X_test\"]\n",
    "\n",
    "y_train = X_train_main.loc[:,\"parallel\"]\n",
    "y_test = X_test_main.loc[:,\"parallel\"]\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"macro\", f1_score(y_pred, y_test.values.ravel(),average='macro'))\n",
    "print(\"micro\", f1_score(y_pred, y_test.values.ravel(),average='micro'))\n",
    "\n",
    "gather_results[\"lr_expanded_19_labels\"] = {\"micro f1\":f1_score(y_pred,y_test,average='micro'), \"macro\":f1_score(y_pred,y_test,average='macro')}\n",
    "\n",
    "print(classification_report(y_pred.ravel(),y_test_main.values.ravel()))\n",
    "\n",
    "\n",
    "gather_data[\"lr_expanded_19_labels\"] = {\"X_train\":X_train,\"y_train\":y_train,\"X_test\":X_test,\"y_test\":y_test,\"model\":model, \"y_pred\":y_pred}\n",
    "\n",
    "heatmap(y_test, y_pred, 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2525151",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gather_data[\"rfr_poly_cluster_expanded\"][\"model\"]\n",
    "\n",
    "X_train = gather_data[\"rfr_poly_cluster_expanded\"][\"X_train\"]\n",
    "X_test = gather_data[\"rfr_poly_cluster_expanded\"][\"X_test\"]\n",
    "\n",
    "y_train = X_train_main.loc[:,\"parallel\"]\n",
    "y_test = X_test_main.loc[:,\"parallel\"]\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"macro\", f1_score(y_pred, y_test.values.ravel(),average='macro'))\n",
    "print(\"micro\", f1_score(y_pred, y_test.values.ravel(),average='micro'))\n",
    "\n",
    "gather_results[\"rfr_poly_cluster_expanded_19_labels\"] = {\"micro f1\":f1_score(y_pred,y_test,average='micro'), \"macro\":f1_score(y_pred,y_test,average='macro')}\n",
    "\n",
    "print(classification_report(y_pred.ravel(),y_test_main.values.ravel()))\n",
    "\n",
    "gather_data[\"rfr_poly_cluster_expanded_19_labels\"] = {\"X_train\":X_train,\"y_train\":y_train,\"X_test\":X_test,\"y_test\":y_test,\"model\":model, \"y_pred\":y_pred}\n",
    "\n",
    "heatmap(y_test, y_pred, 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f50346",
   "metadata": {},
   "source": [
    "##### best model so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b444d584",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = pd.DataFrame.from_dict(gather_results).T.sort_values(by=\"macro\",ascending=False).index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872472c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{best_model} is the best model so far\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8453e266",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gather_data[best_model][\"model\"]\n",
    "\n",
    "X_train = gather_data[best_model][\"X_train\"]#.loc[:, backward_best_features+expanded_cols]\n",
    "# X_train = pd.concat([X_train, tmp1],axis=1)\n",
    "X_test = gather_data[best_model][\"X_test\"]#.loc[:, backward_best_features+expanded_cols]\n",
    "# X_test = pd.concat([X_test, tmp2],axis=1)\n",
    "y_train = gather_data[best_model][\"y_train\"]\n",
    "y_test = gather_data[best_model][\"y_test\"]\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"micro\", f1_score(y_pred, y_test.values.ravel(),average='micro'))\n",
    "print(\"macro\", f1_score(y_pred, y_test.values.ravel(),average='macro'))\n",
    "print(classification_report(y_pred.ravel(),y_test_main.values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9192508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gather_data[best_model][\"model\"]\n",
    "\n",
    "X_train = gather_data[best_model][\"X_train\"].loc[:, forward_best_features+expanded_cols]\n",
    "X_train = pd.concat([X_train, tmp1],axis=1)\n",
    "X_test = gather_databest_model][\"X_test\"].loc[:, forward_best_features+expanded_cols]\n",
    "X_test = pd.concat([X_test, tmp2],axis=1)\n",
    "y_train = gather_data[best_model][\"y_train\"]\n",
    "y_test = gather_data[best_model][\"y_test\"]\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"micro\", f1_score(y_pred, y_test.values.ravel(),average='micro'))\n",
    "print(\"macro\", f1_score(y_pred, y_test.values.ravel(),average='macro'))\n",
    "print(classification_report(y_pred.ravel(),y_test_main.values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6da2afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gather_data[\"rfr_expanded\"][\"model\"]\n",
    "\n",
    "X_train = gather_data[\"rfr_expanded\"][\"X_train\"]\n",
    "X_test = gather_data[\"rfr_expanded\"][\"X_test\"]\n",
    "y_train = gather_data[\"rfr_expanded\"][\"y_train\"]\n",
    "y_test = gather_data[\"rfr_expanded\"][\"y_test\"]\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"micro\", f1_score(y_pred, y_test.values.ravel(),average='micro'))\n",
    "print(\"macro\", f1_score(y_pred, y_test.values.ravel(),average='macro'))\n",
    "print(classification_report(y_pred.ravel(),y_test_main.values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1108cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_compare = y_test.copy()\n",
    "to_compare[\"predictions\"] = y_pred.tolist()\n",
    "wrong_preds = to_compare.loc[to_compare[\"id_on_port\"]!=to_compare[\"predictions\"],:].index\n",
    "to_compare[\"current_port\"] = X_test.loc[X_test.index.isin(wrong_preds),[\"current_port\",]]\n",
    "to_compare = to_compare.dropna()\n",
    "print(f\"{to_compare.shape[0]} were missclassified but it wasn't due to the final number of categories max(port_HAM,port_LA)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdadcec",
   "metadata": {},
   "source": [
    "Hamburg is 0 LA is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e39e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_compare.loc[to_compare[\"current_port\"]==0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372b7c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_compare.loc[to_compare[\"current_port\"]==0,\"predictions\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032f29da",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_compare.loc[to_compare[\"current_port\"]==1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a641485",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_compare.loc[(to_compare[\"current_port\"]==1)& (to_compare[\"predictions\"]>7),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ad0903",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_compare.loc[to_compare[\"current_port\"]==1,\"predictions\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2d866a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# something that completely makes sense since the examples for the terminal 7 and above for each port\n",
    "# have extremely low occurence.\n",
    "\n",
    "# Essentially that plays a role on the misbehaviors of the best performing model so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d753ad",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3e8f55",
   "metadata": {},
   "source": [
    "1 & 2. The features finally selected are coming from the backwards and the forward eliminations. \n",
    "Correlation of the features did give an insight along with logistic regression coefficients and random forests' feature importances, but essentially *correlation does not imply causation* so the safest way to decide was FE and BE.\n",
    "\n",
    "3. My findings agree with the product team since there was a better result by using the actual historical data of each ship to predict the next destination. Before build the model you must check the relationship between the variables as well as their relationship with their target (independent variable).\n",
    "\n",
    "4. It is evident that this is a pure classification problem since we're called to predict essentially a categorical variable.\n",
    "\n",
    "5. Overall my model has a better accuracy than the baseline (thus proving there was room for improvement) but it had worse results from the baseline on getting better results *across all of the terminals*\n",
    "\n",
    "6. The reasons that the algorithm has failed in several occasions is that there were categories with extremely low pupulation i.e. some terminals had up to 10 occurences of data whereas the biggest occurences were found on terminals with over 2000 visits.\n",
    "Also i had 1 model for both LA and HAMBURG so there were bound to be some missclassifications i.e. over the 9nth terminal in LA. Especially since there were significantly less examples in the LA port. Of course post processing here\n",
    "helps the results. One other way that i could have tackled this issue is to actually have 2 models but i believe the one model actually found correlations between some of the terminals for each port.\n",
    "\n",
    "7. The most naive approach i could find here is to actually use the previous visit on the current port as the prediction. Something which proved to be an excellent baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084476d1",
   "metadata": {},
   "source": [
    "## Possible future work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779dbbf9",
   "metadata": {},
   "source": [
    "1. We could try ship embeddings.\n",
    "2. Find a way to see if there is a cap on the occupation of each terminal so we can significantly reduce the search space of our predictions in production.\n",
    "3. Even though i used the \"diff\" feature in the training, it would also be important to get the actual km distance from beggining to the end port.\n",
    "4. We could try binning the \"volume\" of the ships that comes from a histogram as an extra feature.\n",
    "5. Use extensive tuning on non linear ml algorithms but not DNNs since the dataset is quite small.\n",
    "6. Post process by either performing clustering on the terminals (i.e. in case of invalid classification correct it with the closest valid terminal in the same cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c8cda1",
   "metadata": {},
   "source": [
    "A bonus point for discussion:\n",
    "\n",
    "Given the accuracy of your model, what would be the next steps in terms of deploying a solution such\n",
    "that the model’s value to the business is maximised?\n",
    "\n",
    "\n",
    "The first steps i would consider to maximize the business eficiency are:\n",
    "\n",
    "1. Identify how to integrate the model into our workflow and actually set a shadow deployment for it.\n",
    "2. Make sure that we use the proper tools for scaling it for larger datasets (and requests).\n",
    "3. Evaluate the model in the inference by setting up an evaluation platform (this cannot be done manually for a long time).\n",
    "4. Get feedback from the product on any tweaks that it might need to satisfy our customers.\n",
    "5. Set up a plan to make sure it is futureproof and maintainable.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
